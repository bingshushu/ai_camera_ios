import SwiftUI
import AVFoundation

struct AVVideoPlayerView: UIViewRepresentable {
    let player: AVPlayer?
    @Binding var videoSize: CGSize
    let coordinator: VideoPlayerCoordinator
    
    func makeUIView(context: Context) -> AVVideoPlayerUIView {
        let view = AVVideoPlayerUIView()
        view.setPlayer(player)
        view.onSizeChanged = { size in
            DispatchQueue.main.async {
                videoSize = size
            }
        }
        coordinator.screenshotCapture.setAVPlayerView(view)
        return view
    }
    
    func updateUIView(_ uiView: AVVideoPlayerUIView, context: Context) {
        uiView.setPlayer(player)
        coordinator.screenshotCapture.setAVPlayerView(uiView)
    }
}

class AVVideoPlayerUIView: UIView {
    private var playerLayer: AVPlayerLayer?
    var onSizeChanged: ((CGSize) -> Void)?
    
    override init(frame: CGRect) {
        super.init(frame: frame)
        setupView()
    }
    
    required init?(coder: NSCoder) {
        super.init(coder: coder)
        setupView()
    }
    
    private func setupView() {
        backgroundColor = .black
        
        // åˆ›å»ºæ’­æ”¾å™¨å±‚
        playerLayer = AVPlayerLayer()
        playerLayer?.videoGravity = .resizeAspect  // ä¿æŒå®½é«˜æ¯”
        playerLayer?.backgroundColor = UIColor.black.cgColor
        
        if let layer = playerLayer {
            self.layer.addSublayer(layer)
        }
        
        print("ðŸ–¥ï¸ AVPlayer: Video view setup complete")
    }
    
    func setPlayer(_ player: AVPlayer?) {
        print("ðŸ“± AVPlayer: Setting player to view")
        playerLayer?.player = player
    }
    
    override func layoutSubviews() {
        super.layoutSubviews()
        
        // æ›´æ–°æ’­æ”¾å™¨å±‚æ¡†æž¶
        playerLayer?.frame = bounds
        
        // é€šçŸ¥å°ºå¯¸å˜åŒ–
        DispatchQueue.main.async {
            self.onSizeChanged?(self.bounds.size)
        }
    }
    
    // æˆªå›¾åŠŸèƒ½ - çŽ°ä»£å®žçŽ°
    func captureFrame() -> UIImage? {
        guard let player = playerLayer?.player,
              let currentItem = player.currentItem else {
            print("âŒ AVPlayer: No player or current item for capture")
            return nil
        }
        
        let imageGenerator = AVAssetImageGenerator(asset: currentItem.asset)
        imageGenerator.appliesPreferredTrackTransform = true
        imageGenerator.requestedTimeToleranceBefore = .zero
        imageGenerator.requestedTimeToleranceAfter = .zero
        
        // ä½¿ç”¨ä¿¡å·é‡å®žçŽ°åŒæ­¥ç­‰å¾…å¼‚æ­¥ç»“æžœ
        let semaphore = DispatchSemaphore(value: 0)
        var capturedImage: UIImage?
        
        if #available(iOS 18.0, *) {
            // iOS 18+ ä½¿ç”¨æ–°çš„å¼‚æ­¥API
            imageGenerator.generateCGImageAsynchronously(for: player.currentTime()) { cgImage, actualTime, error in
                if let cgImage = cgImage {
                    capturedImage = UIImage(cgImage: cgImage)
                    print("âœ… AVPlayer: Frame captured successfully (async)")
                } else if let error = error {
                    print("âŒ AVPlayer: Frame capture failed (async) - \(error.localizedDescription)")
                }
                semaphore.signal()
            }
            
            // ç­‰å¾…å¼‚æ­¥æ“ä½œå®Œæˆï¼ˆæœ€å¤š1ç§’ï¼‰
            _ = semaphore.wait(timeout: .now() + 1.0)
        } else {
            // iOS 17åŠä»¥ä¸‹ä½¿ç”¨åŒæ­¥API
            do {
                let cgImage = try imageGenerator.copyCGImage(at: player.currentTime(), actualTime: nil)
                capturedImage = UIImage(cgImage: cgImage)
                print("âœ… AVPlayer: Frame captured successfully (sync)")
            } catch {
                print("âŒ AVPlayer: Frame capture failed (sync) - \(error.localizedDescription)")
            }
        }
        
        return capturedImage
    }
    
    // èŽ·å–æ’­æ”¾å™¨å±‚ç”¨äºŽå…¶ä»–æ“ä½œ
    func getPlayerLayer() -> AVPlayerLayer? {
        return playerLayer
    }
}